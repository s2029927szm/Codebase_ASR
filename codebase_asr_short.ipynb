{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66986f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openfst_python as fst\n",
    "import math\n",
    "import glob\n",
    "from subprocess import check_call\n",
    "from IPython.display import Image\n",
    "\n",
    "class CreateWFST:\n",
    "    \n",
    "    def __init__(self, n_phone=3, n_word=1):\n",
    "        self.num_per_phone=n_phone\n",
    "        self.num_per_word=n_word\n",
    "        self.end_weight=fst.Weight('log',-math.log(1))\n",
    "        self.lex=self.parse_lexicon('lexicon.txt')\n",
    "        self.word_table, self.phone_table, self.state_table=self.generate_symbol_tables(self.lex,3)\n",
    "        self.word_table.write_text('word_table.txt')\n",
    "        self.phone_table.write_text('phone_table.txt')\n",
    "        self.state_table.write_text('state_table.txt')\n",
    "        self.unigram_dic={}\n",
    "        self.unigram_dic_cp={}\n",
    "        self.bigram_dic={}\n",
    "        self.bigram_dic_cp={}\n",
    "        self.st_w={}\n",
    "        for word in self.lex.keys():\n",
    "            self.st_w[word]=fst.Weight('log',-math.log(1/len(self.lex.keys())))\n",
    "            \n",
    "    #1***: parse lexicon into a dic: { word: [phones] }\n",
    "    def parse_lexicon(self,lex_file):\n",
    "        lex={}\n",
    "        with open(lex_file,'r') as f:\n",
    "            for line in f:\n",
    "                line=line.split()\n",
    "                if line[0] in lex.keys():\n",
    "                    line[0]=line[0]+'*'\n",
    "                lex[line[0]]=line[1:]\n",
    "        return lex\n",
    "\n",
    "    #2***: generate fst. word_table; phone_table; state_table\n",
    "    def generate_symbol_tables(self,lex_dic, n):\n",
    "        word_table=fst.SymbolTable()\n",
    "        phone_table=fst.SymbolTable()\n",
    "        state_table=fst.SymbolTable()\n",
    "\n",
    "        word_table.add_symbol('<eps>')\n",
    "        phone_table.add_symbol('<eps>')\n",
    "        state_table.add_symbol('<eps>')\n",
    "\n",
    "        for w, ps in lex_dic.items():\n",
    "            word_table.add_symbol(w)\n",
    "\n",
    "            for p in ps:\n",
    "                phone_table.add_symbol(p)\n",
    "                for i in range(1,n+1):\n",
    "                    state_table.add_symbol(\"{}_{}\".format(p,i))\n",
    "\n",
    "        return word_table, phone_table, state_table\n",
    "\n",
    "\n",
    "\n",
    "    #3***: generate phone_n state wfst with final output phone\n",
    "    def generate_phone_sequence_wfst(self, f, start_state, phone, n, sl_w, tr_w):\n",
    "        \n",
    "        current_state=start_state\n",
    "        for i in range(1,n+1):\n",
    "            in_label=self.state_table.find('{}_{}'.format(phone,i))\n",
    "            f.add_arc(current_state, fst.Arc(in_label, 0, sl_w, current_state))\n",
    "            if i==n:\n",
    "                out_label=self.phone_table.find(phone)\n",
    "            else:\n",
    "                out_label=0\n",
    "            next_state=f.add_state()\n",
    "            f.add_arc(current_state, fst.Arc(in_label, out_label, tr_w, next_state))\n",
    "            current_state=next_state\n",
    "        return current_state\n",
    "    \n",
    "    #4***: generate word recognition wfst with phone_n state for each phone,\n",
    "    # phone as output of each word sequence endstate\n",
    "    def generate_word_recognition_wfst(self, n, sl_w, tr_w):\n",
    "        f=fst.Fst('log')\n",
    "        start_state=f.add_state()\n",
    "        f.set_start(start_state)\n",
    "        \n",
    "        #remove_duplicate_phone=set()\n",
    "        #for allphone in self.lex.values():\n",
    "        #    remove_duplicate_phone = remove_duplicate_phone.union(allphone)\n",
    "        \n",
    "        for word, phones in self.lex.items():\n",
    "            current_state=f.add_state()\n",
    "            f.add_arc(start_state, fst.Arc(0, 0, self.st_w[word], current_state))\n",
    "            for phone in phones:\n",
    "                current_state=self.generate_phone_sequence_wfst(f, current_state, phone, n, sl_w, tr_w)\n",
    "            f.set_final(current_state)\n",
    "            f.add_arc(current_state, fst.Arc(0, 0, self.end_weight, start_state))\n",
    "        f.set_input_symbols(self.state_table)\n",
    "        f.set_output_symbols(self.phone_table)\n",
    "        return f\n",
    "    #5***: generate lexicon into wfst\n",
    "    def generate_L_wfst(self):\n",
    "        L=fst.Fst()\n",
    "        start_state=L.add_state()\n",
    "        L.set_start(start_state)\n",
    "        \n",
    "        for word, phones in self.lex.items():\n",
    "            current_state=start_state\n",
    "            for (i,phone) in enumerate(phones):\n",
    "                next_state=L.add_state()\n",
    "                if i==len(phones)-1:\n",
    "                    L.add_arc(current_state, fst.Arc(self.phone_table.find(phone), self.word_table.find(word), None, next_state))\n",
    "                else:\n",
    "                    L.add_arc(current_state, fst.Arc(self.phone_table.find(phone), 0, None, next_state))\n",
    "                current_state=next_state\n",
    "            L.set_final(current_state)\n",
    "            L.add_arc(current_state, fst.Arc(0,0,None,start_state))\n",
    "        L.set_input_symbols(self.phone_table)\n",
    "        L.set_output_symbols(self.word_table)\n",
    "        return L\n",
    "    \n",
    "    #6***: generate linear phone sequence wfst\n",
    "    def generate_linear_phone_wfst(self,seq_list):\n",
    "        P=fst.Fst()\n",
    "        start_state=P.add_state()\n",
    "        P.set_start(start_state)\n",
    "        \n",
    "        for phone in seq_list:\n",
    "            i_label=self.phone_table.find(phone)\n",
    "            current_state=P.add_state()\n",
    "            P.add_arc(start_state, fst.Arc(i_label,i_label,None,current_state))\n",
    "            start_state=current_state\n",
    "        P.set_final(start_state)\n",
    "        P.set_input_symbols(self.phone_table)\n",
    "        P.set_output_symbols(self.phone_table)\n",
    "        return P                    \n",
    "        \n",
    "    #5***:generate word recognition wfst with phone_n state for each phone\n",
    "    # word as final output of each word sequence endstate\n",
    "    def generate_ow_word_recognition_wfst(self, n, sl_w, tr_w):\n",
    "        fw=fst.Fst('log')\n",
    "        start_state=fw.add_state()\n",
    "        fw.set_start(start_state)\n",
    "        \n",
    "        for word, phones in self.lex.items():\n",
    "            current_state=fw.add_state()\n",
    "            fw.add_arc(start_state, fst.Arc(0, 0, self.st_w[word], current_state))\n",
    "            for phone in phones:\n",
    "                for i in range(1, n+1):\n",
    "                    inlabel=self.state_table.find('{}_{}'.format(phone,i))\n",
    "                    fw.add_arc(current_state, fst.Arc(inlabel, 0, sl_w, current_state))\n",
    "                    if i==n and phone==phones[-1]:\n",
    "                        outlabel=self.word_table.find(word)\n",
    "                    else:\n",
    "                        outlabel=0\n",
    "                    next_state=fw.add_state()\n",
    "                    fw.add_arc(current_state, fst.Arc(inlabel, outlabel, tr_w, next_state))\n",
    "                    current_state=next_state\n",
    "            fw.set_final(current_state)\n",
    "            fw.add_arc(current_state, fst.Arc(0, 0, self.end_weight, start_state))\n",
    "        fw.set_input_symbols(self.state_table)\n",
    "        fw.set_output_symbols(self.word_table)\n",
    "        return fw\n",
    "    \n",
    "    #6***: generate word recognition wfst with unigram probability with for each word path\n",
    "    # with phone_n state for each phone\n",
    "    # word as final output of each word sequence end state\n",
    "    def generate_unigram_word_recognition_wfst(self, n, sl_w, tr_w, unigram_dic):\n",
    "        fu=fst.Fst('log')\n",
    "        start_state=fu.add_state()\n",
    "        fu.set_start(start_state)\n",
    "        \n",
    "        for word, phones in self.lex.items():\n",
    "            current_state=fu.add_state()\n",
    "            if '*' in word:\n",
    "                fu.add_arc(start_state, fst.Arc(0, 0, unigram_dic[word.rstrip('*')], current_state))\n",
    "            else:\n",
    "                fu.add_arc(start_state, fst.Arc(0, 0, unigram_dic[word], current_state))\n",
    "            for phone in phones:\n",
    "                for i in range(1, n+1):\n",
    "                    inlabel=self.state_table.find('{}_{}'.format(phone,i))\n",
    "                    fu.add_arc(current_state, fst.Arc(inlabel, 0, sl_w, current_state))\n",
    "                    if i==n and phone==phones[-1]:\n",
    "                        outlabel=self.word_table.find(word)\n",
    "                    else:\n",
    "                        outlabel=0\n",
    "                    next_state=fu.add_state()\n",
    "                    fu.add_arc(current_state, fst.Arc(inlabel, outlabel, tr_w, next_state))\n",
    "                    current_state=next_state\n",
    "            fu.set_final(current_state)\n",
    "            fu.add_arc(current_state, fst.Arc(0, 0, self.end_weight, start_state))\n",
    "        fu.set_input_symbols(self.state_table)\n",
    "        fu.set_output_symbols(self.word_table)\n",
    "        return fu\n",
    "    # generate unigram dic: {'word': 'fst_type' log of probability} P(word_i)= word_i count / total words count\n",
    "    def generate_unigram(self, txt_file_path):\n",
    "        to_transcription=[]\n",
    "        for txt_f in glob.glob(txt_file_path):\n",
    "            with open(txt_f,'r') as f:\n",
    "                tst=f.readline().strip()\n",
    "                to_transcription+=tst.split( )\n",
    "        for token in to_transcription:\n",
    "            if token not in self.unigram_dic.keys():\n",
    "                self.unigram_dic[token]=1\n",
    "            else:\n",
    "                self.unigram_dic[token]+=1\n",
    "        self.unigram_dic_cp=self.unigram_dic.copy()\n",
    "        for word in self.unigram_dic.keys():\n",
    "            self.unigram_dic[word]=fst.Weight('log',-math.log(self.unigram_dic[word]/len(to_transcription)))\n",
    "        return self.unigram_dic\n",
    "    \n",
    "    #7*** genenrate 5 state silence wfst:\n",
    "    def add_silence_wfst(self, afst):\n",
    "        n=5 # per silence state\n",
    "        self.state_table.add_symbol('sil_1')\n",
    "        self.state_table.add_symbol('sil_2')\n",
    "        self.state_table.add_symbol('sil_3')\n",
    "        self.state_table.add_symbol('sil_4')\n",
    "        self.state_table.add_symbol('sil_5')\n",
    "        wei_next=fst.Weight('log',-math.log(0.3))\n",
    "        wei_3=fst.Weight('log',-math.log(1/3))\n",
    "        wei_back=fst.Weight('log',-math.log(0.1))\n",
    "        \n",
    "        start_state=0\n",
    "        afst.set_start(start_state)\n",
    "        st_record=[]\n",
    "        current_state=start_state\n",
    "        \n",
    "        for i in range(1,n+1):\n",
    "            next_state=afst.add_state()\n",
    "            st_record.append(next_state)\n",
    "            if i== 3 or i==4:\n",
    "                in_label= self.state_table.find('{}_{}'.format('sil',i))\n",
    "                afst.add_arc(current_state, fst.Arc(in_label,0,wei_next,next_state))\n",
    "                afst.add_arc(next_state, fst.Arc(in_label,0,wei_next,next_state))\n",
    "                in_label= self.state_table.find('{}_{}'.format('sil',i-1))\n",
    "                afst.add_arc(next_state, fst.Arc(in_label,0,wei_back,current_state))\n",
    "            else:\n",
    "                in_label= self.state_table.find('{}_{}'.format('sil',i))\n",
    "                afst.add_arc(current_state, fst.Arc(in_label,0,wei_3,next_state))\n",
    "                afst.add_arc(next_state, fst.Arc(in_label,0,wei_3,next_state))\n",
    "            current_state=next_state\n",
    "        for i in range(len(st_record)-1):\n",
    "            i_label1=self.state_table.find('sil_5')\n",
    "            if i ==2 or i==3:\n",
    "                afst.add_arc(st_record[i], fst.Arc(i_label1,0,wei_next,next_state))\n",
    "            else:\n",
    "                afst.add_arc(st_record[i], fst.Arc(i_label1,0,wei_3,next_state))\n",
    "        afst.set_final(next_state)\n",
    "        afst.add_arc(next_state, fst.Arc(0,0,self.end_weight,start_state))\n",
    "        afst.set_input_symbols(self.state_table)\n",
    "        #afst.set_output_symbols(self.phone_table)\n",
    "        return afst\n",
    "    \n",
    "    #8***:generate wfst with bigram language model weight\n",
    "    def generate_bigram_word_recognition_wfst(self, n, sl_w, tr_w, unigram_dic, bigram_dic):\n",
    "     \n",
    "        #bigram_dict = generate_bigram_dict(recording_txt_files,lex)\n",
    "\n",
    "        G = fst.Fst('log')\n",
    "        start_state = G.add_state()\n",
    "        G.set_start(start_state)\n",
    "\n",
    "        for wordi, phones in self.lex.items():\n",
    "            current_state=G.add_state()\n",
    "            if '*' in wordi:\n",
    "                G.add_arc(start_state, fst.Arc(0, 0, unigram_dic[wordi.rstrip('*')], current_state))\n",
    "            else:\n",
    "                G.add_arc(start_state, fst.Arc(0, 0, unigram_dic[wordi], current_state))\n",
    "            for phone in phones:\n",
    "                for i in range(1, n+1):\n",
    "                    in_label=self.state_table.find('{}_{}'.format(phone, i))\n",
    "                    G.add_arc(current_state, fst.Arc(in_label, 0, sl_w, current_state))\n",
    "                    if i==n and phone==phones[-1]:\n",
    "                        out_label=self.word_table.find(word)\n",
    "                    else:\n",
    "                        out_label=0\n",
    "                    next_state=G.add_state()\n",
    "                    G.add_arc(current_state, fst.Arc(in_label, out_label, tr_w, next_state))\n",
    "                    current_state=next_state\n",
    "            #add bigram model by change weight here, as we know Sum[ P(w'_i | w_1), ...., P(w'_i | w_n) ]= 1\n",
    "            start_state_ii=current_state\n",
    "            for wordii, phoneiis in self.lex.items():\n",
    "                current_state_ii=G.add_state()\n",
    "                if '*' in wordii:\n",
    "                    wordii=wordii.rstrip('*')\n",
    "                    G.add_arc(start_state_ii, fst.Arc(0, 0, bigram_dic[(wordi, wordii)], current_state_ii))\n",
    "                else:\n",
    "                    G.add_arc(start_state_ii, fst.Arc(0, 0, bigram_dic[(wordi, wordii)], current_state_ii))\n",
    "                for phone in phoneiis:\n",
    "                    for j in range(1, n+1):\n",
    "                        inlabel_ii=self.state_table.find('{}_{}'.format(phone, i))\n",
    "                        G.add_arc(current_state_ii, fst.Arc(inlabel_ii, 0, sl_w, current_state_ii))\n",
    "                        if j==n and phone==phones[-1]:\n",
    "                            outlabel_ii=self.word_table.find(wordii)\n",
    "                        else:\n",
    "                            outlabel_ii=0\n",
    "                        next_state_ii=G.add_state()\n",
    "                        G.add_arc(current_state_ii, fst.Arc(inlabel_ii, outlabel_ii, tr_w, next_state_ii))\n",
    "                        current_state_ii=next_state_ii\n",
    "            G.set_final(current_state_ii)\n",
    "            G.add_arc(current_state_ii, fst.Arc(0, 0, self.end_weight, start_state))\n",
    "        G.set_input_symbols(self.state_table)\n",
    "        G.set_output_symbols(self.word_table)\n",
    "\n",
    "        return G\n",
    "    # compute the bigram dic: {'(w_i, w_i+1)' :  'fst_type' log of probability} P(w_i | w_i+1)= 'w_i, w_i+1' count / w_i count\n",
    "    def generate_bigram(self, txt_file_path):\n",
    "        to_transcription1=[]\n",
    "        for txt_f in glob.glob(txt_file_path):\n",
    "            with open(txt_f,'r') as f:\n",
    "                tst=f.readline().strip()\n",
    "                to_transcription1+=tst.split( )\n",
    "        for ti in range(len(to_transcription1)-1):\n",
    "            if (to_transcription1[ti],to_transcription1[ti+1]) not in self.bigram_dic.keys():\n",
    "                self.bigram_dic[(to_transcription1[ti],to_transcription1[ti+1])]=1\n",
    "            else:\n",
    "                self.bigram_dic[(to_transcription1[ti],to_transcription1[ti+1])]+=1\n",
    "        self.bigram_dic_cp=self.bigram_dic.copy()\n",
    "        for ty in self.bigram_dic_cp:\n",
    "            self.bigram_dic[ty]=fst.Weight('log',-math.log(self.bigram_dic[ty] / self.unigram_dic_cp[ty[0]]))\n",
    "        return self.bigram_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fff5519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import observation_model\n",
    "import math\n",
    "\n",
    "class ViterbiDecoder:\n",
    "    Infinity=1e10 # define a constant represent -log(0)\n",
    "    \n",
    "    def __init__(self, f, audio_file_name):\n",
    "        \n",
    "        self.om=observation_model.ObservationModel()\n",
    "        self.f=f\n",
    "        self.forward_count=0\n",
    "        if audio_file_name:\n",
    "            self.om.load_audio(audio_file_name)\n",
    "        self.initialise_decoding()\n",
    "    \n",
    "    #11***: initialise variables, function calling for assigning initial values\n",
    "    def initialise_decoding(self):\n",
    "        \n",
    "        self.V=[]\n",
    "        self.B=[]\n",
    "        self.W=[]\n",
    "        \n",
    "        for t in range(self.om.observation_length()+1):\n",
    "            self.V.append([self.Infinity]*self.f.num_states()) # V[t: 0->initial, 1~T][state number: 0~N] => values: maxP() or min-Log(P)\n",
    "            self.B.append([-1]*self.f.num_states()) # B[t: 0 will not use, 1~T][state number: 0~N] => values: current state j B[t][j]=last state number i of BEST PATH( min(V[t][i]+ arc[i->j]_weight) )\n",
    "            self.W.append([[] for i in range(self.f.num_states())]) # W[t: 0 will not use, 1~T][state number: 0~N] => values: [output_label] if output <eps> -> [], and the initial all are [] empty list\n",
    "        \n",
    "        self.V[0][self.f.start()]=0.0 #V[0][0]=0.0\n",
    "        self.traverse_epsilon_arcs(0) #actually, t never has value '0'; this will give initil values of V[0][...]\n",
    "        # consider every path (arcm, arcn,...) ends into a state j which's V[0][j]= min(arcm_w, arcn_w, ...)\n",
    "        # consider the normal (word) wfst, state 0 has k arcs means k words (phone sequence sub_wfst)\n",
    "        # maybe one example: V[0][1, 9, 15, 32] (so k here is 4), and the 1~8(8 as end state) is a phone sequence wfst\n",
    "        # after do \"self.traverse_epsilon_arcs(0)\", only V[0][0, 1, 9, 15, 32] has values and V[0][0]=0.0 because\n",
    "        # each V[0][i] will compare and save the smallest one! V[0][0] will compare V[0][8]+ arc[8->0]_weight; & V[0][14]+ arc[14->0]_weight\n",
    "        # so I think whatever that V of (state pointer to state 0) and plus a arc_weight will be greater than just 0.0 => so V[0][0]= 0.0\n",
    "        # Reminder!!! the min path only compare and asign when arc_input_label is <eps>! \" if arc.ilabel==0: \"\n",
    "    \n",
    "    #12***: traverse arcs with <eps> on the input at time t\n",
    "    def traverse_epsilon_arcs(self, t):\n",
    "        \n",
    "        states_to_traverse=list(self.f.states())\n",
    "        \n",
    "        while states_to_traverse:\n",
    "            i=states_to_traverse.pop(0) # always pop out the first (state number) of current state list\n",
    "                                        # start from the first state (maybe 0)\n",
    "            \n",
    "            if self.V[t][i]==self.Infinity:\n",
    "                continue # 0 probability means a disavaliable path\n",
    "            for arc in self.f.arcs(i):\n",
    "                \n",
    "                if arc.ilabel==0: # if this is <eps> transition\n",
    "                    j=arc.nextstate\n",
    "                    \n",
    "                    if self.V[t][j] > self.V[t][i]+float(arc.weight):\n",
    "                        # if this path (at time t, V[t][i]+arc.w less than the original saved cost \n",
    "                        # at time t state, V[t][j])\n",
    "                        # we may want to save this lowwer path to update the probability V[][]\n",
    "                        # also, we need update the previous state number B[][]\n",
    "                        self.V[t][j]= self.V[t][i]+float(arc.weight)\n",
    "                        self.B[t][j]=self.B[t][i]\n",
    "                        self.forward_count+=1\n",
    "                        \n",
    "                        if arc.olabel != 0: # also, if there's output, we should record that.\n",
    "                                            # <eps> input laways have multiple arcs with multiple outputs\n",
    "                                            # if the outputs is <eps>, it just pass this list to state J;\n",
    "                                            # if not add the output label in the tail of the list and pass to state J\n",
    "                            self.W[t][j] = self.W[t][i] + [arc.olabel]\n",
    "                        else:\n",
    "                            self.W[t][j] = self.W[t][i]\n",
    "                        \n",
    "                        if j not in states_to_traverse:\n",
    "                            states_to_traverse.append(j)\n",
    "    \n",
    "    #13***: forward_step:\n",
    "    def forward_step(self, t):\n",
    "        \n",
    "        for i in self.f.states(): # start with the fist state (maybe 0)\n",
    "            if not self.V[t-1][i] == self.Infinity: # because the following assign is on state j, if previous t-1 and last i cost V[t-1][i] is 0 probability, pass by\n",
    "                \n",
    "                for arc in self.f.arcs(i): # arcs: selfloop, next phone_n, other connection\n",
    "                    if arc.ilabel !=0: # <eps> transitions don't emit observation, so the <eps> input arc will not calculate here\n",
    "                        #but the <eps> path will be compared in traverse Fnc, then the end state k (of other paths & <eps> path) 's V[at every t][k] will have the min(-log)\n",
    "                        \n",
    "                        j=arc.nextstate # laways assign next state; the 0 state will be (compare) and assign by all the end states' back arc\n",
    "                        t_p=float(arc.weight) # arc_weigh = transition probability aij (qi->qj)\n",
    "                        e_p= -self.om.log_observation_probability( self.f.input_symbols().find(arc.ilabel), t) # emission probability: P(label | obsevation Ot) label: state represent <=> also equals to (arc points to state j)'s inlabel \n",
    "                        # class om has log_observation_probability object, which input ('string', 'int')\n",
    "                        p_j = t_p + e_p + self.V[t-1][i] # p_j also means temp V[t][j], the path cost in state j at time t = transition_P + emission_P + V[t-1][i] (becasue last time t-1 state i --arc--> current state j) \n",
    "                        \n",
    "                        if p_j < self.V[t][j]: # Viterbi key not sum(path1 + path2), but compare and save min(paths) at each state\n",
    "                            self.V[t][j]=p_j # this assign actually contains V[t-1][last state i], so imagine all the (future t)'s cost V will contain previous (t-1)'s cost; so this is how propagation in HMM (from one state to next time's state)\n",
    "                            self.B[t][j]=i # So we need the best path from: from lower arc's parent state i\n",
    "                            self.forward_count+=1\n",
    "                            \n",
    "                            if arc.olabel !=0: # example n=3, so only the phone_3 state has output: W[any t][phone_3]=[phone label]; else: W[any t][phone_1,2]=[]\n",
    "                                self.W[t][j]=[arc.olabel]\n",
    "                            else:\n",
    "                                self.W[t][j]=[]\n",
    "    #14***: generate finalise_decoding for make no output state in V[T] sets Inifinity value (which we won't except to compare in backtrace: min(V[-1]))\n",
    "    # And we use float(f.final(end_state)) to get the end_state weight (intial 0.0) to add in the V[T][end_state]; float(not end state weight) intial is math.inf\n",
    "    def finalise_decoding(self):\n",
    "        for state in self.f.states():\n",
    "            final_weight=float(self.f.final(state)) #if state isn't final state, output inf\n",
    "            if self.V[-1][state]!=self.Infinity:\n",
    "                if final_weight==math.inf:\n",
    "                    self.V[-1][state]=self.Infinity # inf means the state is not a end_state, so there's no emit\n",
    "                    #so set the cost to infinity, then we won't compare in backtrce\n",
    "                else:\n",
    "                    self.V[-1][state]=self.V[-1][state]+final_weight #if the end state has a weight, it will also be added in V\n",
    "        finished=[x for x in self.V[-1] if x < self.Infinity]\n",
    "        if not finished: #means there's no end_state in the V[T] <=> all state_weight is inf (as initial)\n",
    "            print(\"No path got to the end of the observations.\")\n",
    "    \n",
    "    #15***: decode main\n",
    "    def decode(self):\n",
    "        self.initialise_decoding()\n",
    "        t=1\n",
    "        while t<= self.om.observation_length():\n",
    "            self.forward_step(t)\n",
    "            self.traverse_epsilon_arcs(t)\n",
    "            t+=1\n",
    "        self.finalise_decoding()\n",
    "    \n",
    "    #16***: generate Viterbi algorithm to find minist cost path by backtracing\n",
    "    def backtrace(self):\n",
    "        best_final_state=self.V[-1].index(min(self.V[-1])) # this is min cost state index\n",
    "        best_state_sequence=[best_final_state]\n",
    "        best_out_sequence=[]\n",
    "        \n",
    "        t=self.om.observation_length()\n",
    "        j=best_final_state\n",
    "        \n",
    "        while t>=0: # we can print W[t][j] & j & t to understand\n",
    "            i=self.B[t][j] # as we save in B[t][j] is always the last best state i; here for intuition: there may be several same state here becasue they were in the selfloop, so the previous state is itself.\n",
    "            best_state_sequence.append(i) #here add the connect state number of best path to the list behind, so it needs reverse.\n",
    "            best_out_sequence=self.W[t][j]+best_out_sequence #each output will add in the list front, as it's from T to 1, so the final list will be correct sequence.\n",
    "            j=i\n",
    "            t-=1\n",
    "        \n",
    "        best_state_sequence.reverse() #from 'T to 1' to '1 to T'\n",
    "        best_out_sequence=' '.join([self.f.output_symbols().find(label) for label in best_out_sequence]) #f.output_symbols() as set before is list of words in wfst\n",
    "        \n",
    "        return (best_state_sequence, best_out_sequence)\n",
    "    \n",
    "    #17***: pruning method decode & forward\n",
    "    def decode_pruning(self,p_td):\n",
    "        self.initialise_decoding()\n",
    "        t=1\n",
    "        while t<= self.om.observation_length():\n",
    "            self.forward_step_pruning(t,p_td)\n",
    "            self.traverse_epsilon_arcs(t)\n",
    "            t+=1\n",
    "        self.finalise_decoding()\n",
    "    def forward_step_pruning(self, t, pruning_threshold):\n",
    "        for i in self.f.states(): # start with the fist state (maybe 0)\n",
    "            if not self.V[t-1][i] == self.Infinity: # because the following assign is on state j, if previous t-1 and last i cost V[t-1][i] is 0 probability, pass by\n",
    "                \n",
    "                if self.V[t-1][i]< pruning_threshold: # the path cost is P*P*P... for eahc time, when -log(): it should be v1+v2+v3+...vT\n",
    "                    for arc in self.f.arcs(i): # arcs: selfloop, next phone_n, other connection\n",
    "                        if arc.ilabel !=0: # <eps> transitions don't emit observation, so the <eps> input arc will not calculate here\n",
    "                            #but the <eps> path will be compared in traverse Fnc, then the end state k (of other paths & <eps> path) 's V[at every t][k] will have the min(-log)\n",
    "\n",
    "                            j=arc.nextstate # laways assign next state; the 0 state will be (compare) and assign by all the end states' back arc\n",
    "                            t_p=float(arc.weight) # arc_weigh = transition probability aij (qi->qj)\n",
    "                            e_p= -self.om.log_observation_probability( self.f.input_symbols().find(arc.ilabel), t) # emission probability: P(label | obsevation Ot) label: state represent <=> also equals to (arc points to state j)'s inlabel \n",
    "                            # class om has log_observation_probability object, which input ('string', 'int')\n",
    "                            p_j = t_p + e_p + self.V[t-1][i] # p_j also means temp V[t][j], the path cost in state j at time t = transition_P + emission_P + V[t-1][i] (becasue last time t-1 state i --arc--> current state j) \n",
    "\n",
    "                            if p_j < self.V[t][j]: # Viterbi key not sum(path1 + path2), but compare and save min(paths) at each state\n",
    "                                self.V[t][j]=p_j # this assign actually contains V[t-1][last state i], so imagine all the (future t)'s cost V will contain previous (t-1)'s cost; so this is how propagation in HMM (from one state to next time's state)\n",
    "                                self.B[t][j]=i # So we need the best path from: from lower arc's parent state i\n",
    "                                self.forward_count+=1\n",
    "\n",
    "                                if arc.olabel !=0: # example n=3, so only the phone_3 state has output: W[any t][phone_3]=[phone label]; else: W[any t][phone_1,2]=[]\n",
    "                                    self.W[t][j]=[arc.olabel]\n",
    "                                else:\n",
    "                                    self.W[t][j]=[]\n",
    "    \n",
    "    #17***: Beamsearch method decode & forward\n",
    "    def decode_beamsearch(self,b_td):\n",
    "        self.initialise_decoding()\n",
    "        t=1\n",
    "        while t<= self.om.observation_length():\n",
    "            self.forward_step_beam(t,b_td)\n",
    "            self.traverse_epsilon_arcs(t)\n",
    "            t+=1\n",
    "        self.finalise_decoding()\n",
    "    def forward_step_beam(self, t, beam_threshold):\n",
    "        Vt=[self.V[t-1][i] for i in self.f.states()]#collect all of the probs for states in timestep t-1\n",
    "        beam_s = []\n",
    "        for index, value in sorted(enumerate(Vt), key=lambda x:x[1]):\n",
    "            beam_s.append(index)\n",
    "        beam_s=beam_s[:beam_threshold]\n",
    "        for i in beam_s: # start with the fist state (maybe 0)\n",
    "            if not self.V[t-1][i] == self.Infinity: # because the following assign is on state j, if previous t-1 and last i cost V[t-1][i] is 0 probability, pass by\n",
    "                \n",
    "                for arc in self.f.arcs(i): # arcs: selfloop, next phone_n, other connection\n",
    "                    if arc.ilabel !=0: # <eps> transitions don't emit observation, so the <eps> input arc will not calculate here\n",
    "                        #but the <eps> path will be compared in traverse Fnc, then the end state k (of other paths & <eps> path) 's V[at every t][k] will have the min(-log)\n",
    "                        \n",
    "                        j=arc.nextstate # laways assign next state; the 0 state will be (compare) and assign by all the end states' back arc\n",
    "                        t_p=float(arc.weight) # arc_weigh = transition probability aij (qi->qj)\n",
    "                        e_p= -self.om.log_observation_probability( self.f.input_symbols().find(arc.ilabel), t) # emission probability: P(label | obsevation Ot) label: state represent <=> also equals to (arc points to state j)'s inlabel \n",
    "                        # class om has log_observation_probability object, which input ('string', 'int')\n",
    "                        p_j = t_p + e_p + self.V[t-1][i] # p_j also means temp V[t][j], the path cost in state j at time t = transition_P + emission_P + V[t-1][i] (becasue last time t-1 state i --arc--> current state j) \n",
    "                        \n",
    "                        if p_j < self.V[t][j]: # Viterbi key not sum(path1 + path2), but compare and save min(paths) at each state\n",
    "                            self.V[t][j]=p_j # this assign actually contains V[t-1][last state i], so imagine all the (future t)'s cost V will contain previous (t-1)'s cost; so this is how propagation in HMM (from one state to next time's state)\n",
    "                            self.B[t][j]=i # So we need the best path from: from lower arc's parent state i\n",
    "                            self.forward_count+=1\n",
    "                            \n",
    "                            if arc.olabel !=0: # example n=3, so only the phone_3 state has output: W[any t][phone_3]=[phone label]; else: W[any t][phone_1,2]=[]\n",
    "                                self.W[t][j]=[arc.olabel]\n",
    "                            else:\n",
    "                                self.W[t][j]=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "017a3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import wer\n",
    "import observation_model\n",
    "import timeit\n",
    "import numpy\n",
    "\n",
    "txt_files = '/group/teaching/asr/labs/recordings/*.txt'\n",
    "wav_files = '/group/teaching/asr/labs/recordings/*.wav'\n",
    "\n",
    "#21***: Fnc for get standard transcription from wav matched file\n",
    "def get_transcription(f_wav):\n",
    "    f_txt=os.path.splitext(f_wav)[0]+'.txt'\n",
    "    with open(f_txt, 'r') as f:\n",
    "        transcription=f.readline().strip()\n",
    "    return transcription\n",
    "\n",
    "#22***: Fnc for experiment set up\n",
    "def experiment_set_up(mode, s_w, t_w, pru_td):\n",
    "    wfst=CreateWFST()\n",
    "    L=wfst.generate_L_wfst()\n",
    "    selfloop_weight=fst.Weight('log',-math.log(s_w))\n",
    "    transist_weight=fst.Weight('log',-math.log(t_w))\n",
    "    if mode == 'pru_b_w':\n",
    "        pruning_threshold=float(fst.Weight('log',pru_td))\n",
    "    elif mode == 'beam_b_w':\n",
    "        beam_threshold=int(pru_td)\n",
    "    #v1=ViterbiDecoder()\n",
    "    #1****b_p\n",
    "    if mode == 'b_p':\n",
    "        F=wfst.generate_word_recognition_wfst(3,selfloop_weight,transist_weight) #step1: create wfst\n",
    "        \n",
    "        print(\"Mode: Baseline Recognition, Use Compose To Transfer Recognized Phone Sequecen Into Word Sequence\")\n",
    "        print(\"Phone per state: 3\\nStart Weight: -log(1.0)\\nEnd Weight: initial(0.0)\")\n",
    "        print(\"Selfloop Weight:\",selfloop_weight,\"-log({})\".format(s_w))\n",
    "        print(\"Transist Weight:\",transist_weight,\"-log({})\".format(t_w))\n",
    "        arcs_1=0\n",
    "        states_1=0\n",
    "        for s_1 in F.states():\n",
    "            states_1+=1\n",
    "            for a_1 in F.arcs(s_1):\n",
    "                arcs_1+=1\n",
    "        print(\"Memory Cost: States Count: %d  Arcs Count:%d\"%(states_1,arcs_1))\n",
    "        \n",
    "        decode_time_1=[]\n",
    "        backtrace_time_1=[]\n",
    "        forward_count_1=[]\n",
    "        to_error_count_1=[]\n",
    "        to_transcription_1=[]\n",
    "        for wav in glob.glob(wav_files):\n",
    "            v1=ViterbiDecoder(F,wav)\n",
    "            \n",
    "            start_de_time_1= timeit.default_timer() #decode\n",
    "            v1.decode()\n",
    "            end_de_time_1= timeit.default_timer()\n",
    "            decode_time_1.append(end_de_time_1-start_de_time_1)\n",
    "            \n",
    "            start_ba_time_1= timeit.default_timer() #backtrace\n",
    "            (path_states_1, phone_seq_1)=v1.backtrace()\n",
    "            #print(list(phone_seq_1))\n",
    "            #s=input()\n",
    "            P_1=wfst.generate_linear_phone_wfst(phone_seq_1.split())# input should be list\n",
    "            P_1.arcsort(sort_type='ilabel')\n",
    "            comp_1=fst.compose(P_1,L)\n",
    "            comp_1.project(project_output='True')\n",
    "            comp_1.rmepsilon()\n",
    "            #comp_1.draw('tu.dot',portrait=True)\n",
    "            #check_call(['dot','-Tpng','-Gdpi=1000','tu.dot','-o','tu.png'])\n",
    "            #Image(filename='tu.png')\n",
    "            #s=input()\n",
    "            word_seq_1=[]\n",
    "            for q_1 in comp_1.states():\n",
    "                for arc_1 in comp_1.arcs(q_1):\n",
    "                    word_seq_1.append(comp_1.output_symbols().find(arc_1.olabel))\n",
    "            end_ba_time_1= timeit.default_timer()\n",
    "            backtrace_time_1.append(end_ba_time_1-start_ba_time_1)\n",
    "            forward_count_1.append(v1.forward_count) # after decode, the count should has values\n",
    "            \n",
    "            word_seq_1=' '.join(word_seq_1)\n",
    "            word_seq_1=word_seq_1.replace('*','')\n",
    "            transcription_1=get_transcription(wav)\n",
    "            error_counts_1=wer.compute_alignment_errors(transcription_1,word_seq_1)\n",
    "            WER_1=sum(error_counts_1)/len(transcription_1.split())\n",
    "            to_error_count_1.append(sum(error_counts_1))\n",
    "            to_transcription_1.append(len(transcription_1.split()))\n",
    "            \n",
    "            print(\"\\n\\n*****************************************\")\n",
    "            print(\"File:\",wav,\"\\nSpeed of Viterbi Decoder:\",decode_time_1[-1],\"s\",\" Speed of Backtrace:\",backtrace_time_1[-1],\"s\",\" Forward Count:\",forward_count_1[-1])\n",
    "            #print(\"D:\",(start_de_time_1-end_de_time_1),decode_time_1[-1],\"B:\",start_ba_time_1-end_ba_time_1,backtrace_time_1[-1],\"F:\",v1.forward_count,forward_count_1[-1])\n",
    "            print(\"(Substitutions, Deletions, Insertions) N:\", error_counts_1, len(transcription_1.split()))\n",
    "            print(\"WER:\",WER_1)\n",
    "            print(\"Trascription:\",transcription_1)\n",
    "            print(\"Recognition:\",word_seq_1)\n",
    "        print(\"Average\")\n",
    "        print(\"Total Errors:\",sum(to_error_count_1))\n",
    "        print(\"Total Transcription Words:\",sum(to_transcription_1))\n",
    "        print(\"Total WER:\",sum(to_error_count_1) / sum(to_transcription_1))\n",
    "        print(\"The number of forward computations per wav: \",sum(forward_count_1)/len(forward_count_1))\n",
    "        print(\"Average decode time:\",numpy.mean(decode_time_1))\n",
    "        print(\"Average backtrace time:\",numpy.mean(backtrace_time_1))\n",
    "    \n",
    "    #2****b_w\n",
    "    elif mode == 'b_w':\n",
    "        F_bw=wfst.generate_ow_word_recognition_wfst(3,selfloop_weight,transist_weight) #step1: create specific kind of wfst\n",
    "        \n",
    "        print(\"Mode: Baseline Recognition, WFST set only word label output on its arc path\")\n",
    "        print(\"Phone per state: 3\\nStart Weight: -log(1.0)\\nEnd Weight: initial(0.0)\")\n",
    "        print(\"Selfloop Weight:\",selfloop_weight,\"-log({})\".format(s_w))\n",
    "        print(\"Transist Weight:\",transist_weight,\"-log({})\".format(t_w))\n",
    "        arcs_2=0\n",
    "        states_2=0\n",
    "        for s_2 in F_bw.states():\n",
    "            states_2+=1\n",
    "            for a_2 in F_bw.arcs(s_2):\n",
    "                arcs_2+=1\n",
    "        print(\"Memory Cost: States Count: %d  Arcs Count:%d\"%(states_2,arcs_2))\n",
    "        \n",
    "        decode_time_2=[]\n",
    "        backtrace_time_2=[]\n",
    "        forward_count_2=[]\n",
    "        to_error_count_2=[]\n",
    "        to_transcription_2=[]\n",
    "        for wav_2 in glob.glob(wav_files):\n",
    "            v2=ViterbiDecoder(F_bw,wav_2) #step2: create Viterbi instance for wfst\n",
    "            \n",
    "            start_de_time_2= timeit.default_timer() #step3: decode\n",
    "            v2.decode()\n",
    "            end_de_time_2= timeit.default_timer()\n",
    "            decode_time_2.append(end_de_time_2-start_de_time_2)\n",
    "            \n",
    "            start_ba_time_2= timeit.default_timer() #step4: backtrace\n",
    "            (path_states_2, word_seq_2)=v2.backtrace()\n",
    "            #word_seq_1=[]\n",
    "            \n",
    "            end_ba_time_2= timeit.default_timer()\n",
    "            backtrace_time_2.append(end_ba_time_2-start_ba_time_2)\n",
    "            forward_count_2.append(v2.forward_count) # after decode, the count should has values\n",
    "            \n",
    "            word_seq_2=word_seq_2.replace('*','') #step5: compare result with transcript and comput WER\n",
    "            transcription_2=get_transcription(wav_2)\n",
    "            error_counts_2=wer.compute_alignment_errors(transcription_2,word_seq_2)\n",
    "            WER_2=sum(error_counts_2)/len(transcription_2.split())\n",
    "            to_error_count_2.append(sum(error_counts_2))\n",
    "            to_transcription_2.append(len(transcription_2.split()))\n",
    "            \n",
    "            print(\"\\n\\n*****************************************\")\n",
    "            print(\"File:\",wav_2,\"\\nSpeed of Viterbi Decoder:\",decode_time_2[-1],\"s\",\" Speed of Backtrace:\",backtrace_time_2[-1],\"s\",\" Forward Count:\",forward_count_2[-1])\n",
    "            #print(\"D:\",(start_de_time_1-end_de_time_1),decode_time_1[-1],\"B:\",start_ba_time_1-end_ba_time_1,backtrace_time_1[-1],\"F:\",v1.forward_count,forward_count_1[-1])\n",
    "            print(\"(Substitutions, Deletions, Insertions) N:\", error_counts_2, len(transcription_2.split()))\n",
    "            print(\"WER:\",WER_2)\n",
    "            print(\"Trascription:\",transcription_2)\n",
    "            print(\"Recognition:\",word_seq_2)\n",
    "        print(\"Average\")\n",
    "        print(\"Total Errors:\",sum(to_error_count_2))\n",
    "        print(\"Total Transcription Words:\",sum(to_transcription_2))\n",
    "        print(\"Total WER:\",sum(to_error_count_2) / sum(to_transcription_2))\n",
    "        print(\"The number of forward computations per wav: \",sum(forward_count_2)/len(forward_count_2))\n",
    "        print(\"Average decode time:\",numpy.mean(decode_time_2))\n",
    "        print(\"Average backtrace time:\",numpy.mean(backtrace_time_2))\n",
    "    \n",
    "    #3****uni_w\n",
    "    elif mode == 'uni_w':\n",
    "        #txt_files = '/group/teaching/asr/labs/recordings/*.txt'\n",
    "        uni_dic_3=wfst.generate_unigram(txt_files)\n",
    "        F_uni=wfst.generate_unigram_word_recognition_wfst(3,selfloop_weight,transist_weight,uni_dic_3) #step1: create wfst\n",
    "        \n",
    "        print(\"Mode: Baseline Recognition, WFST set only word label output on its arc path\")\n",
    "        print(\"Phone per state: 3\\nStart Weight: -log(1.0)\\nEnd Weight: initial(0.0)\")\n",
    "        print(\"Selfloop Weight:\",selfloop_weight,\"-log({})\".format(s_w))\n",
    "        print(\"Transist Weight:\",transist_weight,\"-log({})\".format(t_w))\n",
    "        arcs_3=0\n",
    "        states_3=0\n",
    "        for s_3 in F_uni.states():\n",
    "            states_3+=1\n",
    "            for a_3 in F_uni.arcs(s_3):\n",
    "                arcs_3+=1\n",
    "        print(\"Memory Cost: States Count: %d  Arcs Count:%d\"%(states_3,arcs_3))\n",
    "        \n",
    "        decode_time_3=[]\n",
    "        backtrace_time_3=[]\n",
    "        forward_count_3=[]\n",
    "        to_error_count_3=[]\n",
    "        to_transcription_3=[]\n",
    "        for wav_3 in glob.glob(wav_files):\n",
    "            v3=ViterbiDecoder(F_uni,wav_3)\n",
    "            \n",
    "            start_de_time_3= timeit.default_timer() #decode\n",
    "            v3.decode()\n",
    "            end_de_time_3= timeit.default_timer()\n",
    "            decode_time_3.append(end_de_time_3-start_de_time_3)\n",
    "            \n",
    "            start_ba_time_3= timeit.default_timer() #backtrace\n",
    "            (path_states_3, word_seq_3)=v3.backtrace()\n",
    "            #word_seq_1=[]\n",
    "            \n",
    "            end_ba_time_3= timeit.default_timer()\n",
    "            backtrace_time_3.append(end_ba_time_3-start_ba_time_3)\n",
    "            forward_count_3.append(v3.forward_count) # after decode, the count should has values\n",
    "            \n",
    "            word_seq_3=word_seq_3.replace('*','')\n",
    "            transcription_3=get_transcription(wav_3)\n",
    "            error_counts_3=wer.compute_alignment_errors(transcription_3,word_seq_3)\n",
    "            WER_3=sum(error_counts_3)/len(transcription_3.split())\n",
    "            to_error_count_3.append(sum(error_counts_3))\n",
    "            to_transcription_3.append(len(transcription_3.split()))\n",
    "            \n",
    "            print(\"\\n\\n*****************************************\")\n",
    "            print(\"File:\",wav_3,\"\\nSpeed of Viterbi Decoder:\",decode_time_3[-1],\"s\",\" Speed of Backtrace:\",backtrace_time_3[-1],\"s\",\" Forward Count:\",forward_count_3[-1])\n",
    "            #print(\"D:\",(start_de_time_1-end_de_time_1),decode_time_1[-1],\"B:\",start_ba_time_1-end_ba_time_1,backtrace_time_1[-1],\"F:\",v1.forward_count,forward_count_1[-1])\n",
    "            print(\"(Substitutions, Deletions, Insertions) N:\", error_counts_3, len(transcription_3.split()))\n",
    "            print(\"WER:\",WER_3)\n",
    "            print(\"Trascription:\",transcription_3)\n",
    "            print(\"Recognition:\",word_seq_3)\n",
    "        print(\"Average\")\n",
    "        print(\"Total Errors:\",sum(to_error_count_3))\n",
    "        print(\"Total Transcription Words:\",sum(to_transcription_3))\n",
    "        print(\"Total WER:\",sum(to_error_count_3) / sum(to_transcription_3))\n",
    "        print(\"The number of forward computations per wav: \",sum(forward_count_3)/len(forward_count_3))\n",
    "        print(\"Average decode time:\",numpy.mean(decode_time_3))\n",
    "        print(\"Average backtrace time:\",numpy.mean(backtrace_time_3))\n",
    "    \n",
    "    #4*** Pruning implement in baseline \n",
    "    elif mode == 'pru_b_w':\n",
    "        Pru_bw=wfst.generate_ow_word_recognition_wfst(3,selfloop_weight,transist_weight) #step1: create wfst\n",
    "        \n",
    "        print(\"Mode: Baseline Recognition, WFST set only word label output on its arc path\")\n",
    "        print(\"Phone per state: 3\\nStart Weight: -log(1.0)\\nEnd Weight: initial(0.0)\")\n",
    "        print(\"Selfloop Weight:\",selfloop_weight,\"-log({})\".format(s_w))\n",
    "        print(\"Transist Weight:\",transist_weight,\"-log({})\".format(t_w))\n",
    "        arcs_4=0\n",
    "        states_4=0\n",
    "        for s_4 in Pru_bw.states():\n",
    "            states_4+=1\n",
    "            for a_4 in Pru_bw.arcs(s_4):\n",
    "                arcs_4+=1\n",
    "        print(\"Memory Cost: States Count: %d  Arcs Count:%d\"%(states_4,arcs_4))\n",
    "        \n",
    "        decode_time_4=[]\n",
    "        backtrace_time_4=[]\n",
    "        forward_count_4=[]\n",
    "        to_error_count_4=[]\n",
    "        to_transcription_4=[]\n",
    "        for wav_4 in glob.glob(wav_files):\n",
    "            v4=ViterbiDecoder(Pru_bw,wav_4)\n",
    "            \n",
    "            start_de_time_4= timeit.default_timer() #decode\n",
    "            v4.decode_pruning(pruning_threshold)\n",
    "            end_de_time_4= timeit.default_timer()\n",
    "            decode_time_4.append(end_de_time_4-start_de_time_4)\n",
    "            \n",
    "            start_ba_time_4= timeit.default_timer() #backtrace\n",
    "            (path_states_4, word_seq_4)=v4.backtrace()\n",
    "            #word_seq_1=[]\n",
    "            \n",
    "            end_ba_time_4= timeit.default_timer()\n",
    "            backtrace_time_4.append(end_ba_time_4-start_ba_time_4)\n",
    "            forward_count_4.append(v4.forward_count) # after decode, the count should has values\n",
    "            \n",
    "            word_seq_4=word_seq_4.replace('*','')\n",
    "            transcription_4=get_transcription(wav_4)\n",
    "            error_counts_4=wer.compute_alignment_errors(transcription_4,word_seq_4)\n",
    "            WER_4=sum(error_counts_4)/len(transcription_4.split())\n",
    "            to_error_count_4.append(sum(error_counts_4))\n",
    "            to_transcription_4.append(len(transcription_4.split()))\n",
    "            \n",
    "            print(\"\\n\\n*****************************************\")\n",
    "            print(\"File:\",wav_4,\"\\nSpeed of Viterbi Decoder:\",decode_time_4[-1],\"s\",\" Speed of Backtrace:\",backtrace_time_4[-1],\"s\",\" Forward Count:\",forward_count_4[-1])\n",
    "            print(\"(Substitutions, Deletions, Insertions) N:\", error_counts_4, len(transcription_4.split()))\n",
    "            print(\"WER:\",WER_4)\n",
    "            print(\"Trascription:\",transcription_4)\n",
    "            print(\"Recognition:\",word_seq_4)\n",
    "        print(\"Average\")\n",
    "        print(\"Total Errors:\",sum(to_error_count_4))\n",
    "        print(\"Total Transcription Words:\",sum(to_transcription_4))\n",
    "        print(\"Total WER:\",sum(to_error_count_4) / sum(to_transcription_4))\n",
    "        print(\"The number of forward computations per wav: \",sum(forward_count_4)/len(forward_count_4))\n",
    "        print(\"Average decode time:\",numpy.mean(decode_time_4))\n",
    "        print(\"Average backtrace time:\",numpy.mean(backtrace_time_4))\n",
    "    \n",
    "    #5*** Beamsearch implement in baseline \n",
    "    elif mode == 'beam_b_w':\n",
    "        Beam_bw=wfst.generate_ow_word_recognition_wfst(3,selfloop_weight,transist_weight) #step1: create wfst\n",
    "        \n",
    "        print(\"Mode: Baseline Recognition, WFST set only word label output on its arc path\")\n",
    "        print(\"Phone per state: 3\\nStart Weight: -log(1.0)\\nEnd Weight: initial(0.0)\")\n",
    "        print(\"Selfloop Weight:\",selfloop_weight,\"-log({})\".format(s_w))\n",
    "        print(\"Transist Weight:\",transist_weight,\"-log({})\".format(t_w))\n",
    "        arcs_5=0\n",
    "        states_5=0\n",
    "        for s_5 in Beam_bw.states():\n",
    "            states_5+=1\n",
    "            for a_5 in Beam_bw.arcs(s_5):\n",
    "                arcs_5+=1\n",
    "        print(\"Memory Cost: States Count: %d  Arcs Count:%d\"%(states_5,arcs_5))\n",
    "        \n",
    "        decode_time_5=[]\n",
    "        backtrace_time_5=[]\n",
    "        forward_count_5=[]\n",
    "        to_error_count_5=[]\n",
    "        to_transcription_5=[]\n",
    "        for wav_5 in glob.glob(wav_files):\n",
    "            v5=ViterbiDecoder(Beam_bw,wav_5)\n",
    "            \n",
    "            start_de_time_5= timeit.default_timer() #decode\n",
    "            v5.decode_beamsearch(beam_threshold)\n",
    "            end_de_time_5= timeit.default_timer()\n",
    "            decode_time_5.append(end_de_time_5-start_de_time_5)\n",
    "            \n",
    "            start_ba_time_5= timeit.default_timer() #backtrace\n",
    "            (path_states_5, word_seq_5)=v5.backtrace()\n",
    "            #word_seq_1=[]\n",
    "            \n",
    "            end_ba_time_5= timeit.default_timer()\n",
    "            backtrace_time_5.append(end_ba_time_5-start_ba_time_5)\n",
    "            forward_count_5.append(v5.forward_count) # after decode, the count should has values\n",
    "            \n",
    "            word_seq_5=word_seq_5.replace('*','')\n",
    "            transcription_5=get_transcription(wav_5)\n",
    "            error_counts_5=wer.compute_alignment_errors(transcription_5,word_seq_5)\n",
    "            WER_5=sum(error_counts_5)/len(transcription_5.split())\n",
    "            to_error_count_5.append(sum(error_counts_5))\n",
    "            to_transcription_5.append(len(transcription_5.split()))\n",
    "            \n",
    "            print(\"\\n\\n*****************************************\")\n",
    "            print(\"File:\",wav_5,\"\\nSpeed of Viterbi Decoder:\",decode_time_5[-1],\"s\",\" Speed of Backtrace:\",backtrace_time_5[-1],\"s\",\" Forward Count:\",forward_count_5[-1])\n",
    "            print(\"(Substitutions, Deletions, Insertions) N:\", error_counts_5, len(transcription_5.split()))\n",
    "            print(\"WER:\",WER_5)\n",
    "            print(\"Trascription:\",transcription_5)\n",
    "            print(\"Recognition:\",word_seq_5)\n",
    "        print(\"Average\")\n",
    "        print(\"Total Errors:\",sum(to_error_count_5))\n",
    "        print(\"Total Transcription Words:\",sum(to_transcription_5))\n",
    "        print(\"Total WER:\",sum(to_error_count_5) / sum(to_transcription_5))\n",
    "        print(\"The number of forward computations per wav: \",sum(forward_count_5)/len(forward_count_5))\n",
    "        print(\"Average decode time:\",numpy.mean(decode_time_5))\n",
    "        print(\"Average backtrace time:\",numpy.mean(backtrace_time_5))\n",
    "    \n",
    "    #6****add silence to baseline\n",
    "    elif mode == 'sil_b_w':\n",
    "        Sil_bw=wfst.generate_ow_word_recognition_wfst(3,selfloop_weight,transist_weight) #step1: create wfst\n",
    "        Sil_bw=wfst.add_silence_wfst(Sil_bw)\n",
    "        \n",
    "        print(\"Mode: Baseline Recognition, WFST set only word label output on its arc path\")\n",
    "        print(\"Phone per state: 3\\nStart Weight: -log(1.0)\\nEnd Weight: initial(0.0)\")\n",
    "        print(\"Selfloop Weight:\",selfloop_weight,\"-log({})\".format(s_w))\n",
    "        print(\"Transist Weight:\",transist_weight,\"-log({})\".format(t_w))\n",
    "        arcs_6=0\n",
    "        states_6=0\n",
    "        for s_6 in Sil_bw.states():\n",
    "            states_6+=1\n",
    "            for a_6 in Sil_bw.arcs(s_6):\n",
    "                arcs_6+=1\n",
    "        print(\"Memory Cost: States Count: %d  Arcs Count:%d\"%(states_6,arcs_6))\n",
    "        \n",
    "        decode_time_6=[]\n",
    "        backtrace_time_6=[]\n",
    "        forward_count_6=[]\n",
    "        to_error_count_6=[]\n",
    "        to_transcription_6=[]\n",
    "        for wav_6 in glob.glob(wav_files):\n",
    "            v6=ViterbiDecoder(Sil_bw,wav_6)\n",
    "            \n",
    "            start_de_time_6= timeit.default_timer() #decode\n",
    "            v6.decode()\n",
    "            end_de_time_6= timeit.default_timer()\n",
    "            decode_time_6.append(end_de_time_6-start_de_time_6)\n",
    "            \n",
    "            start_ba_time_6= timeit.default_timer() #backtrace\n",
    "            (path_states_6, word_seq_6)=v6.backtrace()\n",
    "            #word_seq_1=[]\n",
    "            \n",
    "            end_ba_time_6= timeit.default_timer()\n",
    "            backtrace_time_6.append(end_ba_time_6-start_ba_time_6)\n",
    "            forward_count_6.append(v6.forward_count) # after decode, the count should has values\n",
    "            \n",
    "            word_seq_6=word_seq_6.replace('*','')\n",
    "            transcription_6=get_transcription(wav_6)\n",
    "            error_counts_6=wer.compute_alignment_errors(transcription_6,word_seq_6)\n",
    "            WER_6=sum(error_counts_6)/len(transcription_6.split())\n",
    "            to_error_count_6.append(sum(error_counts_6))\n",
    "            to_transcription_6.append(len(transcription_6.split()))\n",
    "            \n",
    "            print(\"\\n\\n*****************************************\")\n",
    "            print(\"File:\",wav_6,\"\\nSpeed of Viterbi Decoder:\",decode_time_6[-1],\"s\",\" Speed of Backtrace:\",backtrace_time_6[-1],\"s\",\" Forward Count:\",forward_count_6[-1])\n",
    "            #print(\"D:\",(start_de_time_1-end_de_time_1),decode_time_1[-1],\"B:\",start_ba_time_1-end_ba_time_1,backtrace_time_1[-1],\"F:\",v1.forward_count,forward_count_1[-1])\n",
    "            print(\"(Substitutions, Deletions, Insertions) N:\", error_counts_6, len(transcription_6.split()))\n",
    "            print(\"WER:\",WER_6)\n",
    "            print(\"Trascription:\",transcription_6)\n",
    "            print(\"Recognition:\",word_seq_6)\n",
    "        print(\"Average\")\n",
    "        print(\"Total Errors:\",sum(to_error_count_6))\n",
    "        print(\"Total Transcription Words:\",sum(to_transcription_6))\n",
    "        print(\"Total WER:\",sum(to_error_count_6) / sum(to_transcription_6))\n",
    "        print(\"The number of forward computations per wav: \",sum(forward_count_6)/len(forward_count_6))\n",
    "        print(\"Average decode time:\",numpy.mean(decode_time_6))\n",
    "        print(\"Average backtrace time:\",numpy.mean(backtrace_time_6))\n",
    "    #7****preprocess the lexicon into Tree structure generate wfst\n",
    "    elif mode == 'tree_b_w':\n",
    "        Tree_bw=wfst.generate_ow_word_recognition_wfst(3,selfloop_weight,transist_weight) #step1: create wfst\n",
    "        Tree_bw=fst.determinize(Tree_bw)\n",
    "        \n",
    "        print(\"Mode: Baseline Recognition, WFST set only word label output on its arc path\")\n",
    "        print(\"Phone per state: 3\\nStart Weight: -log(1.0)\\nEnd Weight: initial(0.0)\")\n",
    "        print(\"Selfloop Weight:\",selfloop_weight,\"-log({})\".format(s_w))\n",
    "        print(\"Transist Weight:\",transist_weight,\"-log({})\".format(t_w))\n",
    "        arcs_7=0\n",
    "        states_7=0\n",
    "        for s_7 in Tree_bw.states():\n",
    "            states_7+=1\n",
    "            for a_7 in Tree_bw.arcs(s_7):\n",
    "                arcs_7+=1\n",
    "        print(\"Memory Cost: States Count: %d  Arcs Count:%d\"%(states_7,arcs_7))\n",
    "        \n",
    "        decode_time_7=[]\n",
    "        backtrace_time_7=[]\n",
    "        forward_count_7=[]\n",
    "        to_error_count_7=[]\n",
    "        to_transcription_7=[]\n",
    "        for wav_7 in glob.glob(wav_files):\n",
    "            v7=ViterbiDecoder(Tree_bw,wav_7)\n",
    "            \n",
    "            start_de_time_7= timeit.default_timer() #decode\n",
    "            v7.decode()\n",
    "            end_de_time_7= timeit.default_timer()\n",
    "            decode_time_7.append(end_de_time_7-start_de_time_7)\n",
    "            \n",
    "            start_ba_time_7= timeit.default_timer() #backtrace\n",
    "            (path_states_7, word_seq_7)=v7.backtrace()\n",
    "            #word_seq_1=[]\n",
    "            \n",
    "            end_ba_time_7= timeit.default_timer()\n",
    "            backtrace_time_7.append(end_ba_time_7-start_ba_time_7)\n",
    "            forward_count_7.append(v7.forward_count) # after decode, the count should has values\n",
    "            \n",
    "            word_seq_7=word_seq_7.replace('*','')\n",
    "            transcription_7=get_transcription(wav_7)\n",
    "            error_counts_7=wer.compute_alignment_errors(transcription_7,word_seq_7)\n",
    "            WER_7=sum(error_counts_7)/len(transcription_7.split())\n",
    "            to_error_count_7.append(sum(error_counts_7))\n",
    "            to_transcription_7.append(len(transcription_7.split()))\n",
    "            \n",
    "            print(\"\\n\\n*****************************************\")\n",
    "            print(\"File:\",wav_7,\"\\nSpeed of Viterbi Decoder:\",decode_time_7[-1],\"s\",\" Speed of Backtrace:\",backtrace_time_7[-1],\"s\",\" Forward Count:\",forward_count_7[-1])\n",
    "            #print(\"D:\",(start_de_time_1-end_de_time_1),decode_time_1[-1],\"B:\",start_ba_time_1-end_ba_time_1,backtrace_time_1[-1],\"F:\",v1.forward_count,forward_count_1[-1])\n",
    "            print(\"(Substitutions, Deletions, Insertions) N:\", error_counts_7, len(transcription_7.split()))\n",
    "            print(\"WER:\",WER_7)\n",
    "            print(\"Trascription:\",transcription_7)\n",
    "            print(\"Recognition:\",word_seq_7)\n",
    "        print(\"Average\")\n",
    "        print(\"Total Errors:\",sum(to_error_count_7))\n",
    "        print(\"Total Transcription Words:\",sum(to_transcription_7))\n",
    "        print(\"Total WER:\",sum(to_error_count_7) / sum(to_transcription_7))\n",
    "        print(\"The number of forward computations per wav: \",sum(forward_count_7)/len(forward_count_7))\n",
    "        print(\"Average decode time:\",numpy.mean(decode_time_7))\n",
    "        print(\"Average backtrace time:\",numpy.mean(backtrace_time_7))\n",
    "    #8****b_w\n",
    "    elif mode == 'big_b_w':\n",
    "        Bi_f=wfst.generate_ow_word_recognition_wfst(3,selfloop_weight,transist_weight) #step1: create wfst\n",
    "        uni_dic_8=wfst.generate_unigram(txt_files)\n",
    "        big_dic_8=wfst.generate_bigram(txt_files)\n",
    "        Bi_g=wfst.generate_bigram_word_recognition_wfst(3,selfloop_weight,transist_weight,uni_dic_3,big_dic_8)\n",
    "        #Bi_g=fst.determinize(Bi_g)\n",
    "        #Bi_f=fst.determinize(Bi_f)\n",
    "        Bi_f.arcsort(sort_type ='olabel')\n",
    "        #g=fst.determinize(g)\n",
    "        #f1=fst.determinize(f1)\n",
    "        Bi_bw=fst.compose(Bi_f, Bi_g)\n",
    "        #Bi_bw=fst.determinize(Bi_bw)\n",
    "        #Bi_bw.minimize()\n",
    "        #Bi_bw.rmepsilon()\n",
    "        \n",
    "        print(\"Mode: Baseline Recognition, WFST set only word label output on its arc path\")\n",
    "        print(\"Phone per state: 3\\nStart Weight: -log(1.0)\\nEnd Weight: initial(0.0)\")\n",
    "        print(\"Selfloop Weight:\",selfloop_weight,\"-log({})\".format(s_w))\n",
    "        print(\"Transist Weight:\",transist_weight,\"-log({})\".format(t_w))\n",
    "        arcs_8=0\n",
    "        states_8=0\n",
    "        for s_8 in Bi_bw.states():\n",
    "            states_8+=1\n",
    "            for a_8 in Bi_bw.arcs(s_8):\n",
    "                arcs_8+=1\n",
    "        print(\"Memory Cost: States Count: %d  Arcs Count:%d\"%(states_8,arcs_8))\n",
    "        \n",
    "        decode_time_8=[]\n",
    "        backtrace_time_8=[]\n",
    "        forward_count_8=[]\n",
    "        to_error_count_8=[]\n",
    "        to_transcription_8=[]\n",
    "        for wav_8 in glob.glob(wav_files):\n",
    "            v8=ViterbiDecoder(Bi_bw,wav_8)\n",
    "            \n",
    "            start_de_time_8= timeit.default_timer() #decode\n",
    "            v8.decode()\n",
    "            end_de_time_8= timeit.default_timer()\n",
    "            decode_time_8.append(end_de_time_8-start_de_time_8)\n",
    "            \n",
    "            start_ba_time_8= timeit.default_timer() #backtrace\n",
    "            (path_states_8, word_seq_8)=v8.backtrace()\n",
    "            #word_seq_1=[]\n",
    "            \n",
    "            end_ba_time_8= timeit.default_timer()\n",
    "            backtrace_time_8.append(end_ba_time_8-start_ba_time_8)\n",
    "            forward_count_8.append(v8.forward_count) # after decode, the count should has values\n",
    "            \n",
    "            word_seq_8=word_seq_8.replace('*','')\n",
    "            transcription_8=get_transcription(wav_8)\n",
    "            error_counts_8=wer.compute_alignment_errors(transcription_8,word_seq_8)\n",
    "            WER_8=sum(error_counts_8)/len(transcription_8.split())\n",
    "            to_error_count_8.append(sum(error_counts_8))\n",
    "            to_transcription_8.append(len(transcription_8.split()))\n",
    "            \n",
    "            print(\"\\n\\n*****************************************\")\n",
    "            print(\"File:\",wav_8,\"\\nSpeed of Viterbi Decoder:\",decode_time_8[-1],\"s\",\" Speed of Backtrace:\",backtrace_time_8[-1],\"s\",\" Forward Count:\",forward_count_8[-1])\n",
    "            #print(\"D:\",(start_de_time_1-end_de_time_1),decode_time_1[-1],\"B:\",start_ba_time_1-end_ba_time_1,backtrace_time_1[-1],\"F:\",v1.forward_count,forward_count_1[-1])\n",
    "            print(\"(Substitutions, Deletions, Insertions) N:\", error_counts_8, len(transcription_8.split()))\n",
    "            print(\"WER:\",WER_8)\n",
    "            print(\"Trascription:\",transcription_8)\n",
    "            print(\"Recognition:\",word_seq_8)\n",
    "        print(\"Average\")\n",
    "        print(\"Total Errors:\",sum(to_error_count_8))\n",
    "        print(\"Total Transcription Words:\",sum(to_transcription_8))\n",
    "        print(\"Total WER:\",sum(to_error_count_8) / sum(to_transcription_8))\n",
    "        print(\"The number of forward computations per wav: \",sum(forward_count_8)/len(forward_count_8))\n",
    "        print(\"Average decode time:\",numpy.mean(decode_time_8))\n",
    "        print(\"Average backtrace time:\",numpy.mean(backtrace_time_8))\n",
    "    \n",
    "    return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
